{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import MultiModalDecoder, calc_params\n",
    "from transformers import GPT2LMHeadModel, GPT2Model, GPT2Config, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lora(model):\n",
    "    for layer in model.h:\n",
    "        layer.attn.c_attn = Conv1D_LoRA(\n",
    "            model.config.n_embd, model.config.n_embd * 3, \n",
    "            r               =   model.config.lora_attn_dim, \n",
    "            lora_alpha      =   model.config.lora_attn_alpha, \n",
    "            lora_dropout    =   model.config.lora_dropout, \n",
    "            merge_weights   =   model.config.merge_weights,\n",
    "        )\n",
    "        \n",
    "        if model.config.add_cross_attention:\n",
    "            layer.crossattention.c_attn = Conv1D_LoRA(\n",
    "                model.config.n_embd, model.config.n_embd * 2, \n",
    "                r               =   model.config.lora_attn_dim, \n",
    "                lora_alpha      =   model.config.lora_attn_alpha, \n",
    "                lora_dropout    =   model.config.lora_dropout, \n",
    "                merge_weights   =   model.config.merge_weights,\n",
    "            )\n",
    "\n",
    "            layer.crossattention.q_attn = Conv1D_LoRA(\n",
    "                model.config.n_embd, model.config.n_embd, \n",
    "                r               =   model.config.lora_attn_dim, \n",
    "                lora_alpha      =   model.config.lora_attn_alpha, \n",
    "                lora_dropout    =   model.config.lora_dropout, \n",
    "                merge_weights   =   model.config.merge_weights,\n",
    "            )  \n",
    "        \n",
    "    return model\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "    Args:\n",
    "        nf (`int`): The number of output features.\n",
    "        nx (`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, nx):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "\n",
    "class Conv1D_LoRA(Conv1D, lora.LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int,\n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        Conv1D.__init__(self, out_channels, in_channels, **kwargs)\n",
    "        lora.LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(\n",
    "                self.weight.new_zeros((r, in_channels))\n",
    "            )\n",
    "            self.lora_B = nn.Parameter(\n",
    "                self.weight.new_zeros((out_channels, r))\n",
    "            )\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        Conv1D.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            self.weight.data -= (self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = False\n",
    "    \n",
    "    def eval(self):\n",
    "        Conv1D.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            self.weight.data += (self.lora_B @ self.lora_A) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            size_out = x.size()[:-1] + (self.nf,)\n",
    "            result = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "            result = result.view(size_out)\n",
    "            \n",
    "            if self.r > 0:\n",
    "                result += (self.lora_dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, self.weight, bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.lora_attn_dim = 8\n",
    "config.lora_attn_alpha = 8\n",
    "config.lora_dropout = 0.1\n",
    "config.merge_weights = False\n",
    "config.freeze_pretrained_layers = True\n",
    "config.add_cross_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2Model(config=config)\n",
    "model.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0351,  0.0222, -0.0084,  ...,  0.0049,  0.0164,  0.0307],\n",
       "        [-0.0003,  0.0374, -0.0094,  ..., -0.0145,  0.0292,  0.0094],\n",
       "        [ 0.0148, -0.0177,  0.0074,  ..., -0.0032,  0.0012, -0.0037],\n",
       "        ...,\n",
       "        [-0.0043, -0.0013, -0.0250,  ..., -0.0226,  0.0216, -0.0330],\n",
       "        [-0.0525,  0.0031, -0.0011,  ..., -0.0094, -0.0126,  0.0471],\n",
       "        [ 0.0233, -0.0420,  0.0165,  ..., -0.0037, -0.0274, -0.0042]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained('gpt2', config=config)\n",
    "model.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "        ...,\n",
       "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2-large')\n",
    "config.lora_attn_dim = 8\n",
    "config.lora_attn_alpha = 8\n",
    "config.lora_dropout = 0.1\n",
    "config.merge_weights = False\n",
    "config.freeze_pretrained_layers = True\n",
    "config.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D_LoRA(\n",
       "      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (crossattention): GPT2Attention(\n",
       "    (c_attn): Conv1D_LoRA(\n",
       "      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (q_attn): Conv1D_LoRA(\n",
       "      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_cross_attn): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2Model(config=config)\n",
    "model = set_lora(model)\n",
    "model.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-large and are newly initialized: ['h.23.crossattention.q_attn.weight', 'h.14.crossattention.c_proj.weight', 'h.13.ln_cross_attn.weight', 'h.29.crossattention.masked_bias', 'h.17.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.24.crossattention.q_attn.weight', 'h.11.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.18.ln_cross_attn.weight', 'h.21.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.3.crossattention.bias', 'h.7.crossattention.q_attn.weight', 'h.20.crossattention.masked_bias', 'h.33.crossattention.c_proj.bias', 'h.32.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.17.crossattention.c_proj.bias', 'h.26.crossattention.masked_bias', 'h.31.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.weight', 'h.14.crossattention.c_attn.bias', 'h.26.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.17.crossattention.c_attn.bias', 'h.21.ln_cross_attn.bias', 'h.25.crossattention.c_proj.weight', 'h.13.crossattention.q_attn.weight', 'h.24.crossattention.c_proj.weight', 'h.20.crossattention.bias', 'h.33.crossattention.c_attn.bias', 'h.17.ln_cross_attn.bias', 'h.21.crossattention.bias', 'h.4.crossattention.c_proj.weight', 'h.16.crossattention.c_proj.weight', 'h.15.ln_cross_attn.weight', 'h.14.crossattention.masked_bias', 'h.33.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.22.ln_cross_attn.bias', 'h.14.crossattention.bias', 'h.34.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.34.crossattention.c_proj.bias', 'h.30.crossattention.c_proj.weight', 'h.25.crossattention.masked_bias', 'h.17.crossattention.c_proj.weight', 'h.31.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.15.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.27.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.21.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.21.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.23.crossattention.c_attn.bias', 'h.28.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.12.crossattention.q_attn.bias', 'h.17.crossattention.bias', 'h.7.crossattention.c_attn.weight', 'h.17.ln_cross_attn.weight', 'h.28.crossattention.c_attn.weight', 'h.18.crossattention.c_proj.bias', 'h.9.crossattention.masked_bias', 'h.33.crossattention.bias', 'h.23.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.35.crossattention.q_attn.weight', 'h.33.crossattention.masked_bias', 'h.29.crossattention.c_proj.bias', 'h.1.crossattention.c_attn.bias', 'h.13.ln_cross_attn.bias', 'h.35.ln_cross_attn.bias', 'h.13.crossattention.masked_bias', 'h.30.crossattention.bias', 'h.13.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.34.ln_cross_attn.bias', 'h.10.crossattention.bias', 'h.28.ln_cross_attn.weight', 'h.31.crossattention.masked_bias', 'h.5.ln_cross_attn.weight', 'h.14.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.21.crossattention.q_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.33.ln_cross_attn.bias', 'h.16.ln_cross_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.30.crossattention.q_attn.weight', 'h.27.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.12.crossattention.masked_bias', 'h.2.crossattention.q_attn.weight', 'h.24.crossattention.c_proj.bias', 'h.30.ln_cross_attn.weight', 'h.0.crossattention.bias', 'h.18.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.29.crossattention.q_attn.bias', 'h.14.crossattention.q_attn.bias', 'h.20.crossattention.q_attn.weight', 'h.29.crossattention.c_attn.bias', 'h.10.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.15.ln_cross_attn.bias', 'h.29.crossattention.c_attn.weight', 'h.35.crossattention.q_attn.bias', 'h.28.crossattention.bias', 'h.24.crossattention.bias', 'h.19.crossattention.masked_bias', 'h.31.crossattention.bias', 'h.12.ln_cross_attn.weight', 'h.12.crossattention.c_attn.bias', 'h.8.ln_cross_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.bias', 'h.2.crossattention.masked_bias', 'h.35.crossattention.c_attn.bias', 'h.33.ln_cross_attn.weight', 'h.3.crossattention.masked_bias', 'h.30.crossattention.masked_bias', 'h.1.crossattention.masked_bias', 'h.31.ln_cross_attn.bias', 'h.16.crossattention.c_proj.bias', 'h.22.crossattention.q_attn.bias', 'h.6.ln_cross_attn.bias', 'h.34.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.35.crossattention.bias', 'h.4.ln_cross_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.16.crossattention.c_attn.bias', 'h.12.crossattention.c_proj.weight', 'h.1.ln_cross_attn.bias', 'h.33.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.weight', 'h.20.crossattention.c_proj.bias', 'h.30.crossattention.q_attn.bias', 'h.21.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.35.crossattention.c_proj.bias', 'h.23.crossattention.c_proj.weight', 'h.16.crossattention.c_attn.weight', 'h.12.crossattention.q_attn.weight', 'h.26.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.17.crossattention.masked_bias', 'h.1.crossattention.c_proj.bias', 'h.30.crossattention.c_proj.bias', 'h.18.crossattention.q_attn.weight', 'h.13.crossattention.c_attn.weight', 'h.22.crossattention.c_attn.bias', 'h.23.ln_cross_attn.weight', 'h.22.crossattention.q_attn.weight', 'h.26.crossattention.c_attn.bias', 'h.5.crossattention.masked_bias', 'h.31.ln_cross_attn.weight', 'h.29.crossattention.c_proj.weight', 'h.14.ln_cross_attn.weight', 'h.19.crossattention.bias', 'h.11.crossattention.q_attn.bias', 'h.0.crossattention.masked_bias', 'h.31.crossattention.c_attn.weight', 'h.13.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.weight', 'h.35.crossattention.c_attn.weight', 'h.16.crossattention.q_attn.weight', 'h.35.ln_cross_attn.weight', 'h.4.crossattention.masked_bias', 'h.14.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.32.crossattention.masked_bias', 'h.21.crossattention.q_attn.bias', 'h.28.ln_cross_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.26.ln_cross_attn.weight', 'h.28.crossattention.q_attn.bias', 'h.13.crossattention.c_attn.bias', 'h.8.ln_cross_attn.weight', 'h.16.ln_cross_attn.weight', 'h.32.ln_cross_attn.weight', 'h.26.ln_cross_attn.bias', 'h.18.crossattention.c_attn.bias', 'h.30.crossattention.c_attn.bias', 'h.25.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.18.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.12.crossattention.c_attn.weight', 'h.20.crossattention.c_attn.weight', 'h.34.crossattention.masked_bias', 'h.5.crossattention.q_attn.bias', 'h.10.crossattention.masked_bias', 'h.19.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.32.crossattention.c_proj.weight', 'h.34.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.5.ln_cross_attn.bias', 'h.27.crossattention.bias', 'h.19.ln_cross_attn.bias', 'h.18.crossattention.masked_bias', 'h.6.crossattention.q_attn.bias', 'h.30.crossattention.c_attn.weight', 'h.22.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.33.crossattention.q_attn.bias', 'h.34.crossattention.bias', 'h.12.crossattention.bias', 'h.22.crossattention.c_proj.bias', 'h.35.crossattention.masked_bias', 'h.22.crossattention.bias', 'h.29.ln_cross_attn.bias', 'h.34.crossattention.c_attn.weight', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.bias', 'h.28.crossattention.c_proj.bias', 'h.27.crossattention.c_proj.bias', 'h.7.crossattention.bias', 'h.25.crossattention.c_attn.weight', 'h.18.ln_cross_attn.bias', 'h.29.ln_cross_attn.weight', 'h.15.crossattention.q_attn.weight', 'h.24.crossattention.c_attn.bias', 'h.17.crossattention.q_attn.bias', 'h.32.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.bias', 'h.19.crossattention.q_attn.weight', 'h.16.crossattention.bias', 'h.21.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.6.crossattention.bias', 'h.11.crossattention.bias', 'h.7.ln_cross_attn.bias', 'h.22.ln_cross_attn.weight', 'h.32.crossattention.q_attn.weight', 'h.27.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.16.crossattention.masked_bias', 'h.13.crossattention.c_proj.bias', 'h.24.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.24.crossattention.q_attn.bias', 'h.27.ln_cross_attn.bias', 'h.26.crossattention.c_proj.bias', 'h.15.crossattention.bias', 'h.22.crossattention.c_proj.weight', 'h.27.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.19.crossattention.c_attn.weight', 'h.4.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.18.crossattention.bias', 'h.2.crossattention.bias', 'h.33.crossattention.q_attn.weight', 'h.22.crossattention.masked_bias', 'h.24.ln_cross_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.25.ln_cross_attn.bias', 'h.28.crossattention.c_proj.weight', 'h.32.crossattention.q_attn.bias', 'h.32.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.19.ln_cross_attn.weight', 'h.6.crossattention.masked_bias', 'h.26.crossattention.bias', 'h.32.crossattention.bias', 'h.32.ln_cross_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.25.crossattention.q_attn.weight', 'h.31.crossattention.c_proj.bias', 'h.19.crossattention.c_attn.bias', 'h.18.crossattention.c_proj.weight', 'h.25.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.14.crossattention.c_proj.bias', 'h.15.crossattention.q_attn.bias', 'h.20.crossattention.q_attn.bias', 'h.27.crossattention.c_attn.bias', 'h.34.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.25.crossattention.q_attn.bias', 'h.15.crossattention.c_attn.bias', 'h.10.ln_cross_attn.weight', 'h.23.crossattention.q_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.25.crossattention.bias', 'h.14.crossattention.c_attn.weight', 'h.11.ln_cross_attn.bias', 'h.7.crossattention.masked_bias', 'h.11.crossattention.q_attn.weight', 'h.17.crossattention.c_attn.weight', 'h.1.crossattention.bias', 'h.29.crossattention.bias', 'h.15.crossattention.c_attn.weight', 'h.20.crossattention.c_proj.weight', 'h.13.crossattention.bias', 'h.11.crossattention.c_attn.bias', 'h.25.crossattention.c_proj.bias', 'h.28.crossattention.masked_bias', 'h.19.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.bias', 'h.24.crossattention.masked_bias', 'h.11.crossattention.c_proj.weight', 'h.34.ln_cross_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.26.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.19.crossattention.q_attn.bias', 'h.31.crossattention.q_attn.weight', 'h.12.crossattention.c_proj.bias', 'h.23.crossattention.c_proj.bias', 'h.8.crossattention.masked_bias', 'h.15.crossattention.c_proj.bias', 'h.21.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.weight', 'h.23.crossattention.c_attn.weight', 'h.27.crossattention.q_attn.bias', 'h.20.ln_cross_attn.bias', 'h.0.ln_cross_attn.bias', 'h.20.ln_cross_attn.weight', 'h.20.crossattention.c_attn.bias', 'h.31.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.12.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.23.crossattention.bias', 'h.8.crossattention.bias', 'h.16.crossattention.q_attn.bias', 'h.30.ln_cross_attn.bias', 'h.15.crossattention.masked_bias', 'h.26.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.35.crossattention.c_proj.weight', 'h.28.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.24.ln_cross_attn.weight', 'h.23.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.9.ln_cross_attn.weight', 'h.27.crossattention.masked_bias', 'h.29.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['h.0.attn.c_attn.lora_A', 'h.0.attn.c_attn.lora_B', 'h.0.crossattention.c_attn.lora_A', 'h.0.crossattention.c_attn.lora_B', 'h.0.crossattention.q_attn.lora_A', 'h.0.crossattention.q_attn.lora_B', 'h.1.attn.c_attn.lora_A', 'h.1.attn.c_attn.lora_B', 'h.1.crossattention.c_attn.lora_A', 'h.1.crossattention.c_attn.lora_B', 'h.1.crossattention.q_attn.lora_A', 'h.1.crossattention.q_attn.lora_B', 'h.2.attn.c_attn.lora_A', 'h.2.attn.c_attn.lora_B', 'h.2.crossattention.c_attn.lora_A', 'h.2.crossattention.c_attn.lora_B', 'h.2.crossattention.q_attn.lora_A', 'h.2.crossattention.q_attn.lora_B', 'h.3.attn.c_attn.lora_A', 'h.3.attn.c_attn.lora_B', 'h.3.crossattention.c_attn.lora_A', 'h.3.crossattention.c_attn.lora_B', 'h.3.crossattention.q_attn.lora_A', 'h.3.crossattention.q_attn.lora_B', 'h.4.attn.c_attn.lora_A', 'h.4.attn.c_attn.lora_B', 'h.4.crossattention.c_attn.lora_A', 'h.4.crossattention.c_attn.lora_B', 'h.4.crossattention.q_attn.lora_A', 'h.4.crossattention.q_attn.lora_B', 'h.5.attn.c_attn.lora_A', 'h.5.attn.c_attn.lora_B', 'h.5.crossattention.c_attn.lora_A', 'h.5.crossattention.c_attn.lora_B', 'h.5.crossattention.q_attn.lora_A', 'h.5.crossattention.q_attn.lora_B', 'h.6.attn.c_attn.lora_A', 'h.6.attn.c_attn.lora_B', 'h.6.crossattention.c_attn.lora_A', 'h.6.crossattention.c_attn.lora_B', 'h.6.crossattention.q_attn.lora_A', 'h.6.crossattention.q_attn.lora_B', 'h.7.attn.c_attn.lora_A', 'h.7.attn.c_attn.lora_B', 'h.7.crossattention.c_attn.lora_A', 'h.7.crossattention.c_attn.lora_B', 'h.7.crossattention.q_attn.lora_A', 'h.7.crossattention.q_attn.lora_B', 'h.8.attn.c_attn.lora_A', 'h.8.attn.c_attn.lora_B', 'h.8.crossattention.c_attn.lora_A', 'h.8.crossattention.c_attn.lora_B', 'h.8.crossattention.q_attn.lora_A', 'h.8.crossattention.q_attn.lora_B', 'h.9.attn.c_attn.lora_A', 'h.9.attn.c_attn.lora_B', 'h.9.crossattention.c_attn.lora_A', 'h.9.crossattention.c_attn.lora_B', 'h.9.crossattention.q_attn.lora_A', 'h.9.crossattention.q_attn.lora_B', 'h.10.attn.c_attn.lora_A', 'h.10.attn.c_attn.lora_B', 'h.10.crossattention.c_attn.lora_A', 'h.10.crossattention.c_attn.lora_B', 'h.10.crossattention.q_attn.lora_A', 'h.10.crossattention.q_attn.lora_B', 'h.11.attn.c_attn.lora_A', 'h.11.attn.c_attn.lora_B', 'h.11.crossattention.c_attn.lora_A', 'h.11.crossattention.c_attn.lora_B', 'h.11.crossattention.q_attn.lora_A', 'h.11.crossattention.q_attn.lora_B', 'h.12.attn.c_attn.lora_A', 'h.12.attn.c_attn.lora_B', 'h.12.crossattention.c_attn.lora_A', 'h.12.crossattention.c_attn.lora_B', 'h.12.crossattention.q_attn.lora_A', 'h.12.crossattention.q_attn.lora_B', 'h.13.attn.c_attn.lora_A', 'h.13.attn.c_attn.lora_B', 'h.13.crossattention.c_attn.lora_A', 'h.13.crossattention.c_attn.lora_B', 'h.13.crossattention.q_attn.lora_A', 'h.13.crossattention.q_attn.lora_B', 'h.14.attn.c_attn.lora_A', 'h.14.attn.c_attn.lora_B', 'h.14.crossattention.c_attn.lora_A', 'h.14.crossattention.c_attn.lora_B', 'h.14.crossattention.q_attn.lora_A', 'h.14.crossattention.q_attn.lora_B', 'h.15.attn.c_attn.lora_A', 'h.15.attn.c_attn.lora_B', 'h.15.crossattention.c_attn.lora_A', 'h.15.crossattention.c_attn.lora_B', 'h.15.crossattention.q_attn.lora_A', 'h.15.crossattention.q_attn.lora_B', 'h.16.attn.c_attn.lora_A', 'h.16.attn.c_attn.lora_B', 'h.16.crossattention.c_attn.lora_A', 'h.16.crossattention.c_attn.lora_B', 'h.16.crossattention.q_attn.lora_A', 'h.16.crossattention.q_attn.lora_B', 'h.17.attn.c_attn.lora_A', 'h.17.attn.c_attn.lora_B', 'h.17.crossattention.c_attn.lora_A', 'h.17.crossattention.c_attn.lora_B', 'h.17.crossattention.q_attn.lora_A', 'h.17.crossattention.q_attn.lora_B', 'h.18.attn.c_attn.lora_A', 'h.18.attn.c_attn.lora_B', 'h.18.crossattention.c_attn.lora_A', 'h.18.crossattention.c_attn.lora_B', 'h.18.crossattention.q_attn.lora_A', 'h.18.crossattention.q_attn.lora_B', 'h.19.attn.c_attn.lora_A', 'h.19.attn.c_attn.lora_B', 'h.19.crossattention.c_attn.lora_A', 'h.19.crossattention.c_attn.lora_B', 'h.19.crossattention.q_attn.lora_A', 'h.19.crossattention.q_attn.lora_B', 'h.20.attn.c_attn.lora_A', 'h.20.attn.c_attn.lora_B', 'h.20.crossattention.c_attn.lora_A', 'h.20.crossattention.c_attn.lora_B', 'h.20.crossattention.q_attn.lora_A', 'h.20.crossattention.q_attn.lora_B', 'h.21.attn.c_attn.lora_A', 'h.21.attn.c_attn.lora_B', 'h.21.crossattention.c_attn.lora_A', 'h.21.crossattention.c_attn.lora_B', 'h.21.crossattention.q_attn.lora_A', 'h.21.crossattention.q_attn.lora_B', 'h.22.attn.c_attn.lora_A', 'h.22.attn.c_attn.lora_B', 'h.22.crossattention.c_attn.lora_A', 'h.22.crossattention.c_attn.lora_B', 'h.22.crossattention.q_attn.lora_A', 'h.22.crossattention.q_attn.lora_B', 'h.23.attn.c_attn.lora_A', 'h.23.attn.c_attn.lora_B', 'h.23.crossattention.c_attn.lora_A', 'h.23.crossattention.c_attn.lora_B', 'h.23.crossattention.q_attn.lora_A', 'h.23.crossattention.q_attn.lora_B', 'h.24.attn.c_attn.lora_A', 'h.24.attn.c_attn.lora_B', 'h.24.crossattention.c_attn.lora_A', 'h.24.crossattention.c_attn.lora_B', 'h.24.crossattention.q_attn.lora_A', 'h.24.crossattention.q_attn.lora_B', 'h.25.attn.c_attn.lora_A', 'h.25.attn.c_attn.lora_B', 'h.25.crossattention.c_attn.lora_A', 'h.25.crossattention.c_attn.lora_B', 'h.25.crossattention.q_attn.lora_A', 'h.25.crossattention.q_attn.lora_B', 'h.26.attn.c_attn.lora_A', 'h.26.attn.c_attn.lora_B', 'h.26.crossattention.c_attn.lora_A', 'h.26.crossattention.c_attn.lora_B', 'h.26.crossattention.q_attn.lora_A', 'h.26.crossattention.q_attn.lora_B', 'h.27.attn.c_attn.lora_A', 'h.27.attn.c_attn.lora_B', 'h.27.crossattention.c_attn.lora_A', 'h.27.crossattention.c_attn.lora_B', 'h.27.crossattention.q_attn.lora_A', 'h.27.crossattention.q_attn.lora_B', 'h.28.attn.c_attn.lora_A', 'h.28.attn.c_attn.lora_B', 'h.28.crossattention.c_attn.lora_A', 'h.28.crossattention.c_attn.lora_B', 'h.28.crossattention.q_attn.lora_A', 'h.28.crossattention.q_attn.lora_B', 'h.29.attn.c_attn.lora_A', 'h.29.attn.c_attn.lora_B', 'h.29.crossattention.c_attn.lora_A', 'h.29.crossattention.c_attn.lora_B', 'h.29.crossattention.q_attn.lora_A', 'h.29.crossattention.q_attn.lora_B', 'h.30.attn.c_attn.lora_A', 'h.30.attn.c_attn.lora_B', 'h.30.crossattention.c_attn.lora_A', 'h.30.crossattention.c_attn.lora_B', 'h.30.crossattention.q_attn.lora_A', 'h.30.crossattention.q_attn.lora_B', 'h.31.attn.c_attn.lora_A', 'h.31.attn.c_attn.lora_B', 'h.31.crossattention.c_attn.lora_A', 'h.31.crossattention.c_attn.lora_B', 'h.31.crossattention.q_attn.lora_A', 'h.31.crossattention.q_attn.lora_B', 'h.32.attn.c_attn.lora_A', 'h.32.attn.c_attn.lora_B', 'h.32.crossattention.c_attn.lora_A', 'h.32.crossattention.c_attn.lora_B', 'h.32.crossattention.q_attn.lora_A', 'h.32.crossattention.q_attn.lora_B', 'h.33.attn.c_attn.lora_A', 'h.33.attn.c_attn.lora_B', 'h.33.crossattention.c_attn.lora_A', 'h.33.crossattention.c_attn.lora_B', 'h.33.crossattention.q_attn.lora_A', 'h.33.crossattention.q_attn.lora_B', 'h.34.attn.c_attn.lora_A', 'h.34.attn.c_attn.lora_B', 'h.34.crossattention.c_attn.lora_A', 'h.34.crossattention.c_attn.lora_B', 'h.34.crossattention.q_attn.lora_A', 'h.34.crossattention.q_attn.lora_B', 'h.35.attn.c_attn.lora_A', 'h.35.attn.c_attn.lora_B', 'h.35.crossattention.c_attn.lora_A', 'h.35.crossattention.c_attn.lora_B', 'h.35.crossattention.q_attn.lora_A', 'h.35.crossattention.q_attn.lora_B'], unexpected_keys=[])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(GPT2Model.from_pretrained('gpt2-large', config=config).state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0149, -0.0209,  0.0021,  ...,  0.0336, -0.0005, -0.0090],\n",
       "        [ 0.0055, -0.0438,  0.0013,  ...,  0.0671,  0.0329, -0.0399],\n",
       "        [ 0.0585,  0.0603,  0.0302,  ..., -0.1041, -0.0566, -0.0330],\n",
       "        ...,\n",
       "        [-0.0108, -0.0908,  0.0624,  ..., -0.0337,  0.0777,  0.0293],\n",
       "        [ 0.0195, -0.0318,  0.0182,  ...,  0.0191, -0.0454, -0.0139],\n",
       "        [-0.0419,  0.0848, -0.0512,  ..., -0.0083, -0.0447, -0.0274]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1655,  0.1230,  0.1003,  ..., -0.0081,  0.0106, -0.0183],\n",
       "        [-0.2344,  0.1413,  0.0706,  ..., -0.0105,  0.0239, -0.0101],\n",
       "        [ 0.1063, -0.0397,  0.1085,  ..., -0.0042,  0.0183, -0.0080],\n",
       "        ...,\n",
       "        [ 0.0020,  0.1257, -0.0798,  ...,  0.0024,  0.0351,  0.0204],\n",
       "        [-0.1146, -0.0897, -0.0925,  ...,  0.0013,  0.0007, -0.0041],\n",
       "        [ 0.0222, -0.0171, -0.0463,  ...,  0.0290,  0.0258, -0.0327]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0149, -0.0345,  0.0357,  ..., -0.0123, -0.0229, -0.0260],\n",
      "        [-0.0354,  0.0232, -0.0051,  ..., -0.0033,  0.0278,  0.0222],\n",
      "        [-0.0041,  0.0141,  0.0063,  ..., -0.0153, -0.0293, -0.0105],\n",
      "        ...,\n",
      "        [-0.0215,  0.0144,  0.0012,  ...,  0.0281,  0.0139, -0.0212],\n",
      "        [-0.0192,  0.0033,  0.0161,  ...,  0.0365,  0.0213,  0.0209],\n",
      "        [ 0.0221, -0.0083, -0.0060,  ...,  0.0314, -0.0185,  0.0160]])\n"
     ]
    }
   ],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0088,  0.0222,  0.0117,  ..., -0.0116, -0.0125,  0.0190],\n",
       "        [-0.0096,  0.0007,  0.0225,  ..., -0.0149, -0.0054, -0.0150],\n",
       "        [-0.0007, -0.0119, -0.0269,  ...,  0.0207, -0.0078,  0.0135],\n",
       "        ...,\n",
       "        [-0.0053,  0.0191, -0.0040,  ..., -0.0259,  0.0180, -0.0130],\n",
       "        [-0.0069,  0.0077,  0.0231,  ...,  0.0128, -0.0095,  0.0179],\n",
       "        [-0.0258,  0.0083,  0.0131,  ..., -0.0107, -0.0210, -0.0210]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].attn.c_attn.lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.h[0].attn.c_attn.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-2.7540e-02, -1.3705e-02, -2.5356e-02,  ..., -1.5621e-02,\n",
      "          1.7183e-02,  4.5410e-03],\n",
      "        [-2.1208e-02,  2.2249e-02,  8.3704e-03,  ...,  2.0344e-02,\n",
      "          3.9501e-05,  1.4097e-02],\n",
      "        [ 1.7279e-02,  7.2668e-03, -2.3049e-02,  ..., -1.6981e-02,\n",
      "         -1.6657e-02, -2.2538e-02],\n",
      "        ...,\n",
      "        [-1.2725e-02, -1.8495e-02, -6.5629e-03,  ..., -1.6573e-02,\n",
      "         -5.2309e-03,  7.3493e-03],\n",
      "        [ 2.1535e-02,  1.9177e-02,  2.5598e-02,  ..., -1.7152e-02,\n",
      "          1.9801e-02, -6.6687e-03],\n",
      "        [ 2.4029e-02,  2.2541e-02, -8.1920e-04,  ...,  2.2911e-02,\n",
      "         -8.7083e-03, -2.4797e-02]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.c_attn.lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.c_attn.lora_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0151,  0.0266, -0.0242,  ..., -0.0091, -0.0085,  0.0243],\n",
      "        [-0.0064,  0.0069,  0.0012,  ..., -0.0268,  0.0254,  0.0026],\n",
      "        [ 0.0011,  0.0042,  0.0096,  ..., -0.0152, -0.0240, -0.0101],\n",
      "        ...,\n",
      "        [ 0.0237, -0.0232, -0.0014,  ...,  0.0060,  0.0163,  0.0102],\n",
      "        [-0.0102, -0.0076, -0.0234,  ...,  0.0265,  0.0042, -0.0258],\n",
      "        [-0.0106,  0.0184, -0.0261,  ..., -0.0229, -0.0084, -0.0051]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.q_attn.lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.h[0].crossattention.q_attn.lora_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        param += p.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659659520"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora.mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        param += p.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3317760"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.LongTensor([[3,2,1,3],[5,1,2312,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6968e-01,  1.9771e-01,  8.4906e-02,  ..., -4.3974e-01,\n",
       "          -1.7160e-01, -1.4506e+00],\n",
       "         [-1.8743e-01, -3.9547e-02, -4.5775e-01,  ...,  1.6749e-01,\n",
       "          -1.7147e-01, -9.6216e-01],\n",
       "         [ 1.8001e-01, -3.7222e-02,  4.5877e-01,  ..., -2.7406e-01,\n",
       "           1.0326e-01, -1.3773e+00],\n",
       "         [-1.1983e-01, -9.5554e-01, -4.7077e-02,  ..., -4.0969e-01,\n",
       "           1.9316e-01, -2.1377e+00]],\n",
       "\n",
       "        [[-2.5236e-01, -1.0755e-01, -1.0459e-01,  ..., -4.0032e-01,\n",
       "           2.6567e-02, -1.8530e+00],\n",
       "         [-2.7021e-01, -3.1398e-04,  5.7317e-02,  ..., -3.8848e-01,\n",
       "          -5.2567e-01, -2.0494e+00],\n",
       "         [ 2.5673e-02, -2.2973e-01,  2.4588e-01,  ..., -2.3121e-01,\n",
       "           3.8505e-01, -8.1887e-01],\n",
       "         [-3.3591e-02, -3.3863e-01,  1.6547e-02,  ..., -5.7154e-01,\n",
       "          -1.5595e-01, -1.8523e+00]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['last_hidden_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.ln_cross_attn.bias', 'h.5.crossattention.c_attn.bias', 'h.11.crossattention.bias', 'h.10.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.5.crossattention.bias', 'h.0.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.0.crossattention.masked_bias', 'h.8.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.3.crossattention.masked_bias', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.8.ln_cross_attn.bias', 'h.11.crossattention.masked_bias', 'h.11.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.1.crossattention.masked_bias', 'h.2.crossattention.q_attn.bias', 'h.6.ln_cross_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.0.crossattention.bias', 'h.7.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.c_proj.weight', 'h.1.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.2.ln_cross_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.3.crossattention.bias', 'h.3.crossattention.q_attn.weight', 'h.2.ln_cross_attn.weight', 'h.9.crossattention.bias', 'h.5.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.2.crossattention.masked_bias', 'h.6.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.10.crossattention.bias', 'h.0.ln_cross_attn.weight', 'h.6.crossattention.masked_bias', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.bias', 'h.4.crossattention.bias', 'h.6.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.2.crossattention.bias', 'h.2.crossattention.c_proj.bias', 'h.1.crossattention.bias', 'h.7.crossattention.masked_bias', 'h.3.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.bias', 'h.7.crossattention.bias', 'h.5.crossattention.c_proj.weight', 'h.2.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.4.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.1.crossattention.c_attn.bias', 'h.4.ln_cross_attn.weight', 'h.10.crossattention.masked_bias', 'h.4.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.weight', 'h.8.crossattention.bias', 'h.4.crossattention.c_attn.bias', 'h.11.ln_cross_attn.bias', 'h.9.crossattention.masked_bias', 'h.8.crossattention.masked_bias', 'h.5.crossattention.masked_bias', 'h.10.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.bias', 'h.1.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build a Multimodal Decoder with LoRA\n",
      "Trainable parameters of a Multimodal Decoder change 111002880 to 663552\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.lora_attn_dim = 8\n",
    "config.lora_attn_alpha = 8\n",
    "config.lora_dropout = 0.1\n",
    "config.merge_weights = False\n",
    "config.freeze_pretrained_layers = True\n",
    "config.add_cross_attention = True\n",
    "\n",
    "multimodal_decoder = MultiModalDecoder(config=config)\n",
    "multimodal_decoder = set_lora(multimodal_decoder)\n",
    "multimodal_decoder.load_state_dict(\n",
    "    GPT2Model.from_pretrained('gpt2', config=config).state_dict(), \n",
    "    strict=False\n",
    ")\n",
    "\n",
    "print('Build a Multimodal Decoder with LoRA')\n",
    "before_param = calc_params(multimodal_decoder)\n",
    "\n",
    "# set lora grad\n",
    "lora.mark_only_lora_as_trainable(multimodal_decoder)\n",
    "\n",
    "after_param = calc_params(multimodal_decoder)\n",
    "print('Trainable parameters of a Multimodal Decoder change {} to {}'.format(before_param, after_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build a Unimodal Decoder with LoRA\n",
      "Trainable parameters of a Unimodal Decoder change 103501056 to 294912\n"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.lora_attn_dim = 8\n",
    "config.lora_attn_alpha = 8\n",
    "config.lora_dropout = 0.1\n",
    "config.merge_weights = False\n",
    "config.freeze_pretrained_layers = True\n",
    "config.add_cross_attention = False\n",
    "\n",
    "unimodal_decoder = GPT2Model(config=config)\n",
    "unimodal_decoder = set_lora(unimodal_decoder)\n",
    "unimodal_decoder.load_state_dict(\n",
    "    GPT2Model.from_pretrained('gpt2', config=config).state_dict(), \n",
    "    strict=False\n",
    ")\n",
    "\n",
    "print('Build a Unimodal Decoder with LoRA')\n",
    "before_param = calc_params(unimodal_decoder)\n",
    "\n",
    "# set lora grad\n",
    "lora.mark_only_lora_as_trainable(unimodal_decoder)\n",
    "\n",
    "after_param = calc_params(unimodal_decoder)\n",
    "print('Trainable parameters of a Unimodal Decoder change {} to {}'.format(before_param, after_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = unimodal_decoder(torch.LongTensor([[123,52,512],[7345,34,124]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 0.4528,  1.8306, -0.5822,  ..., -0.1598,  0.6187,  0.1820],\n",
       "         [ 0.2870,  0.2701, -0.6151,  ..., -0.0373,  0.6674,  0.2033],\n",
       "         [ 0.3178,  0.7613, -0.2611,  ...,  0.0581,  0.7786, -0.0921]],\n",
       "\n",
       "        [[ 0.0738,  1.6187,  0.1168,  ..., -0.2524, -0.0491, -0.2290],\n",
       "         [ 0.4228,  1.5954,  0.2552,  ..., -0.4613,  0.0583, -0.2242],\n",
       "         [ 0.7592,  1.3706,  0.0426,  ..., -0.6588,  0.1929, -0.0444]]],\n",
       "       grad_fn=<ViewBackward>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_decoder(\n",
    "    hidden_states         = output['last_hidden_state'],\n",
    "    attention_mask        = torch.ones((2,3)),\n",
    "    encoder_hidden_states = torch.randn(2,4,768)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
