{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import loralib as lora\n",
    "import math\n",
    "from transformers import GPT2LMHeadModel, GPT2Model, GPT2Config, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lora(model):\n",
    "    for layer in model.transformer.h:\n",
    "        layer.attn.c_attn = Conv1D_LoRA(\n",
    "            model.config.n_embd, model.config.n_embd * 3, \n",
    "            r               =   model.config.lora_attn_dim, \n",
    "            lora_alpha      =   model.config.lora_attn_alpha, \n",
    "            lora_dropout    =   model.config.lora_dropout, \n",
    "            merge_weights   =   model.config.merge_weights,\n",
    "        )\n",
    "        \n",
    "        if model.config.add_cross_attention:\n",
    "            layer.crossattention.c_attn = Conv1D_LoRA(\n",
    "                model.config.n_embd, model.config.n_embd * 2, \n",
    "                r               =   model.config.lora_attn_dim, \n",
    "                lora_alpha      =   model.config.lora_attn_alpha, \n",
    "                lora_dropout    =   model.config.lora_dropout, \n",
    "                merge_weights   =   model.config.merge_weights,\n",
    "            )\n",
    "\n",
    "            layer.crossattention.q_attn = Conv1D_LoRA(\n",
    "                model.config.n_embd, model.config.n_embd, \n",
    "                r               =   model.config.lora_attn_dim, \n",
    "                lora_alpha      =   model.config.lora_attn_alpha, \n",
    "                lora_dropout    =   model.config.lora_dropout, \n",
    "                merge_weights   =   model.config.merge_weights,\n",
    "            )  \n",
    "        \n",
    "    return model\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D-convolutional layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2).\n",
    "    Basically works like a linear layer but the weights are transposed.\n",
    "    Args:\n",
    "        nf (`int`): The number of output features.\n",
    "        nx (`int`): The number of input features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nf, nx):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "\n",
    "class Conv1D_LoRA(Conv1D, lora.LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int,\n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        Conv1D.__init__(self, out_channels, in_channels, **kwargs)\n",
    "        lora.LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(\n",
    "                self.weight.new_zeros((r, in_channels))\n",
    "            )\n",
    "            self.lora_B = nn.Parameter(\n",
    "                self.weight.new_zeros((out_channels, r))\n",
    "            )\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize A the same way as the default for nn.Linear and B to zero\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        Conv1D.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # Make sure that the weights are not merged\n",
    "            self.weight.data -= (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling\n",
    "            self.merged = False\n",
    "    \n",
    "    def eval(self):\n",
    "        Conv1D.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # Merge the weights and mark it\n",
    "            self.weight.data += (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling\n",
    "            self.merged = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            return F.Conv1d(\n",
    "                x, \n",
    "                self.weight + (self.lora_B @ self.lora_A).view(self.weight.shape) * self.scaling,\n",
    "                self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "            )\n",
    "        return Conv1D.forward(self, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.lora_attn_dim = 8\n",
    "config.lora_attn_alpha = 8\n",
    "config.lora_dropout = 0.1\n",
    "config.merge_weights = False\n",
    "config.freeze_pretrained_layers = True\n",
    "config.add_cross_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config=config)\n",
    "model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.4167e-03, -3.9373e-02, -7.9190e-04,  ...,  3.7932e-02,\n",
       "         -2.2393e-02, -1.8256e-02],\n",
       "        [ 6.2204e-03,  2.1795e-02,  1.6698e-03,  ...,  1.2985e-02,\n",
       "         -2.4400e-02, -1.3343e-02],\n",
       "        [-6.9144e-03, -2.8654e-02,  3.1401e-02,  ...,  1.5245e-02,\n",
       "          2.6001e-02, -5.7864e-04],\n",
       "        ...,\n",
       "        [ 4.2912e-02,  2.5906e-02, -1.0334e-02,  ..., -1.6790e-02,\n",
       "          1.6977e-02, -3.6761e-02],\n",
       "        [ 1.1425e-02, -1.7903e-05,  5.6524e-03,  ...,  2.7493e-02,\n",
       "         -1.1056e-02,  1.7742e-02],\n",
       "        [ 6.8175e-03, -1.0026e-02, -7.7820e-03,  ...,  2.8702e-03,\n",
       "          3.3572e-03, -3.0546e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0045, -0.0025, -0.0126,  ...,  0.0088,  0.0105,  0.0001],\n",
       "        [-0.0025, -0.0063,  0.0039,  ...,  0.0008,  0.0022,  0.0093],\n",
       "        [ 0.0102,  0.0097,  0.0131,  ..., -0.0013,  0.0159,  0.0037],\n",
       "        ...,\n",
       "        [ 0.0159,  0.0258, -0.0146,  ...,  0.0278, -0.0046, -0.0224],\n",
       "        [-0.0096,  0.0167, -0.0005,  ..., -0.0083, -0.0074, -0.0398],\n",
       "        [ 0.0262, -0.0165, -0.0196,  ..., -0.0243,  0.0262,  0.0109]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
    "model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "        ...,\n",
       "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D_LoRA(\n",
       "      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config=config)\n",
    "model = set_lora(model)\n",
    "model.transformer.h[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['transformer.h.0.attn.c_attn.lora_A', 'transformer.h.0.attn.c_attn.lora_B', 'transformer.h.1.attn.c_attn.lora_A', 'transformer.h.1.attn.c_attn.lora_B', 'transformer.h.2.attn.c_attn.lora_A', 'transformer.h.2.attn.c_attn.lora_B', 'transformer.h.3.attn.c_attn.lora_A', 'transformer.h.3.attn.c_attn.lora_B', 'transformer.h.4.attn.c_attn.lora_A', 'transformer.h.4.attn.c_attn.lora_B', 'transformer.h.5.attn.c_attn.lora_A', 'transformer.h.5.attn.c_attn.lora_B', 'transformer.h.6.attn.c_attn.lora_A', 'transformer.h.6.attn.c_attn.lora_B', 'transformer.h.7.attn.c_attn.lora_A', 'transformer.h.7.attn.c_attn.lora_B', 'transformer.h.8.attn.c_attn.lora_A', 'transformer.h.8.attn.c_attn.lora_B', 'transformer.h.9.attn.c_attn.lora_A', 'transformer.h.9.attn.c_attn.lora_B', 'transformer.h.10.attn.c_attn.lora_A', 'transformer.h.10.attn.c_attn.lora_B', 'transformer.h.11.attn.c_attn.lora_A', 'transformer.h.11.attn.c_attn.lora_B'], unexpected_keys=[])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(GPT2LMHeadModel.from_pretrained('gpt2', config=config).state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
       "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
       "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
       "        ...,\n",
       "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
       "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
       "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "        ...,\n",
       "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.c_attn.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0356,  0.0222,  0.0258,  ..., -0.0347, -0.0331,  0.0075],\n",
       "        [ 0.0211,  0.0300, -0.0234,  ...,  0.0281, -0.0070,  0.0213],\n",
       "        [-0.0181, -0.0181,  0.0236,  ..., -0.0028, -0.0094, -0.0066],\n",
       "        ...,\n",
       "        [-0.0266, -0.0205,  0.0049,  ..., -0.0171, -0.0159,  0.0267],\n",
       "        [-0.0302,  0.0024,  0.0270,  ...,  0.0355,  0.0327, -0.0252],\n",
       "        [ 0.0339, -0.0072, -0.0233,  ..., -0.0155, -0.0185, -0.0092]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].attn.c_attn.lora_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.c_attn.lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.c_attn.lora_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.q_attn.lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.add_cross_attention:\n",
    "    print(model.transformer.h[0].crossattention.q_attn.lora_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        param += p.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103501056"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora.mark_only_lora_as_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        param += p.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294912"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
